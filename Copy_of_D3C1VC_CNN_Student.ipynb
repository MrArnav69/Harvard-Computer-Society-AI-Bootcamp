{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLjWPialMbBQ"
      },
      "source": [
        "#Harvard AI Bootcamp - Handwriting Classification with Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdR8-cp-9aGh"
      },
      "source": [
        "## Make a copy of this notebook! Editing directly will not be saved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPVZ2bsPMr78"
      },
      "source": [
        "##Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRk_IUlKM3jV"
      },
      "source": [
        "Welcome to this interactive project on Handwriting Classification using Convolutional Neural Networks (CNNs). In this tutorial, we'll explore the powerful capabilities of CNNs in the context of recognizing handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbGEII9C7h_v"
      },
      "source": [
        "##Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQajiW8y7lic"
      },
      "source": [
        "Let's start by installing the required dependencies. Google Colab comes pre-installed with many essential libraries, but we need to ensure that PyTorch is installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTcZ_33z7sEH",
        "outputId": "026a263e-ce4e-4ef8-e3b1-95bf325fdac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
            "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.20.1)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYlcmRW2NGm2"
      },
      "source": [
        "##Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve58vdR-NLUJ"
      },
      "source": [
        "Convolutional Neural Networks have emerged as a cornerstone in computer vision, particularly excelling in tasks like image classification. Designed to simulate the visual processing of the human brain, CNNs have proven to be highly effective in discerning intricate patterns and features within images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p55HjK9nNN7O"
      },
      "source": [
        "##Application: Handwriting Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jgFQhj6NUpL"
      },
      "source": [
        "In this project, our focus is on a classic yet vital application—recognizing handwritten digits. We'll be working with the MNIST dataset, a benchmark dataset in the machine learning community. Despite its simplicity, MNIST remains a challenging problem due to the wide variability in individual handwriting styles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ink8r0fNW06"
      },
      "source": [
        "##Importance of CNNs in Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXdoC-coNZsS"
      },
      "source": [
        "Understanding CNNs is essential in numerous real-world applications, from digit recognition in postal services to automated form processing. By concentrating on the specific task of handwriting classification, we aim to showcase how CNNs can robustly handle diverse and intricate datasets, laying the foundation for broader applications in pattern recognition.\n",
        "\n",
        "Now, let's embark on the journey of setting up our project and constructing a CNN model to proficiently classify handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZvQtavHNcXP"
      },
      "source": [
        "#Setup Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGNcFDU1Nobo"
      },
      "source": [
        "In this tutorial, we will be using PyTorch to build a Convolutional Neural Network (CNN) for classifying handwritten digits from the MNIST dataset. Before we dive into the model building process, let's set up our environment by installing the required dependencies and importing the necessary libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uq-CzswP2ce"
      },
      "source": [
        "##Why PyTorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WmpB0YcP5Z9"
      },
      "source": [
        "PyTorch is a popular open-source deep learning framework that provides a flexible and dynamic computational graph. It is widely used for building and training neural networks due to its ease of use, dynamic computation capabilities, and strong community support.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK_z58CSQFl0"
      },
      "source": [
        "##Step 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gZkMF6nEQLXk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C2ygRjhQQwa"
      },
      "source": [
        "*   **'torch'**: The core PyTorch library.\n",
        "*   **'torch.nn'**: PyTorch's module for defining neural network architectures.  \n",
        "*   **'torch.optim'**: PyTorch's module for optimization algorithms.\n",
        "*   **'torchvision'**: PyTorch's library for computer vision tasks.\n",
        "*   **['datasets'](https://docs.pytorch.org/vision/main/datasets.html)**: Provides pre-loaded datasets, including MNIST.\n",
        "*   **['transforms'](https://docs.pytorch.org/vision/main/transforms.html)**: Allows us to define image transformations, such as normalization and resizing.\n",
        "*   **'torch.nn.functional'**: module that provides a collection of functions that operate on tensors and are commonly used in neural network operations. These functions include various activation functions, loss functions, and other operations that are applied element-wise to tensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvmihCOQSrNU"
      },
      "source": [
        "##Step 3: Data Preprocessing with Transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANwk2jlSStrL"
      },
      "source": [
        "MNIST dataset contains grayscale images of handwritten digits. We will use transformations to preprocess the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-3tX06TPSvd3"
      },
      "outputs": [],
      "source": [
        "# Define data transforms\n",
        "# TODO: convert images to PyTorch tensors to have a mean of 0.5 and a standard deviation of 0.5.\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNl2X1auTCo_"
      },
      "source": [
        "\n",
        "\n",
        "*   **'ToTensor()'**: Converts images to PyTorch tensors.  \n",
        "*   **'Normalize()'**: Normalizes pixel values to have a mean of 0.5 and a standard deviation of 0.5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZyj0kycTSbH"
      },
      "source": [
        "##Step 4: Download and Load MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGsu6Q8mTWMU"
      },
      "source": [
        "Let's download the MNIST dataset and apply the defined transforms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dvj55zcTYzm",
        "outputId": "30f2a63a-a676-4e9c-a28a-327f78fbc44c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
          ]
        }
      ],
      "source": [
        "# Download and load training set\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "# Download and load test set\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_RwPAlOTlEP"
      },
      "source": [
        "\n",
        "\n",
        "*   **'datasets.MNIST'**: Loads the MNIST dataset. If you look under the 'data' folder on the right, you should be able to see MNIST loaded in.\n",
        "*   **'root'**: Specifies the directory to save the dataset.\n",
        "*   **'train=True'**: Loads the training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y8V1X2JUMWv"
      },
      "source": [
        "##Setup Section Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAm9u-stUPe7"
      },
      "source": [
        "With these setup steps, we have created an environment ready for building our CNN for handwritten digit classification using PyTorch. In the upcoming sections, we will delve into constructing the neural network architecture, training the model, and evaluating its performance.\n",
        "\n",
        "Let's proceed to the next section and start building our CNN!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRSvQvfPU0gW"
      },
      "source": [
        "#Building the CNN Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VTLTtlCU3Am"
      },
      "source": [
        "Now that we have set up our environment and loaded the MNIST dataset, let's move on to building the Convolutional Neural Network (CNN) for classifying handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpP0m3liUuW-"
      },
      "source": [
        "##Step 5: Define the CNN Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMwKloG4U56P"
      },
      "source": [
        "We will create a simple CNN architecture for this task. In this example, we'll use two convolutional layers followed by max-pooling, and then two fully connected (linear) layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgXCmqGGU8Qb"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        # TODO: similarly create another convolutional layer\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        # TODO: similarly create another fully connected layer\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7) #reshape x\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBKIWBKKU-2V"
      },
      "source": [
        "This architecture consists of two convolutional layers, each followed by max-pooling, and two fully connected layers. The forward method defines the forward pass of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB8rOLqzVCj1"
      },
      "source": [
        "##Step 6: Instantiate the Model and Define Loss Function & Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdJmEpeRVFvz"
      },
      "source": [
        "Now, let's instantiate the CNN model and define the loss function and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkiZCQXtVJb5"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "model = CNNClassifier()\n",
        "\n",
        "# TODO: Define an Adam optimizer and specify the learning rate as 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGbud00lVLsr"
      },
      "source": [
        "\n",
        "\n",
        "*   **'nn.CrossEntropyLoss()'**: This is a commonly used loss function for classification problems.\n",
        "*   **'optim.Adam()'**: We use the Adam optimizer for updating the model parameters during training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkS62Ge0VYOl"
      },
      "source": [
        "##Step 7: Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_0h7CbhVaSr"
      },
      "source": [
        "Let's train the model using the training dataset we loaded earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kgUFvTPVc5V",
        "outputId": "441ed906-ebe2-4f2d-c532-3fea60065d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1], Loss: 0.1555, Accuracy: 95.34%\n"
          ]
        }
      ],
      "source": [
        "# Defne batch size for training\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Set the number of epochs\n",
        "num_epochs = 1 #this is too low, but given that we are teaching this in real time we will use 1 epoch for speed\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # TODO: set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct_train, total_train = 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        # TODO: Zero the gradients\n",
        "        # TODO: Forward pass\n",
        "        # TODO: Calculate the loss\n",
        "        # TODO: Backward pass\n",
        "        # TODO: Update weights\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy_train = correct_train / total_train\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy_train * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF8XYHm9WBRO"
      },
      "source": [
        "##Step 8: Evaluate the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLttrIBQWDdB"
      },
      "source": [
        "After training, let's evaluate the model on the test set to assess it's performance. One popular visualzation of a model's performance is a confusion matrix. To do this, we'll need to use the **'sklearn.metrics'** module, which provides convenient functions for calculating and displaying various metrics.\n",
        "\n",
        "First make sure to install scikit-learn if you haven't already:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYvDcNtbbO-2",
        "outputId": "f99bd0ac-06f9-446e-a9e9-cb3d2e9c1676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-U-x2YabQ9N"
      },
      "source": [
        "Now let's modify the import statements to include the calculation and printing of the confusion matrix and classificaiton report:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLlSld5sbUuh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WBzhab7WHct",
        "outputId": "d4096b1a-c021-4e66-c603-dac7c32233e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 98.36%\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 973    0    1    0    0    1    2    2    1    0]\n",
            " [   0 1131    1    0    0    1    1    0    1    0]\n",
            " [   0    2 1025    0    1    0    0    3    1    0]\n",
            " [   0    0    9  981    0   12    0    5    2    1]\n",
            " [   0    0    1    0  975    0    1    0    1    4]\n",
            " [   2    0    0    3    0  885    2    0    0    0]\n",
            " [   4    1    0    0    2    4  946    0    1    0]\n",
            " [   0    2    9    1    0    1    0 1008    1    6]\n",
            " [   5    1   11    0    4    7    7    2  932    5]\n",
            " [   1    1    1    0   11    7    0    7    1  980]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       980\n",
            "           1       0.99      1.00      1.00      1135\n",
            "           2       0.97      0.99      0.98      1032\n",
            "           3       1.00      0.97      0.98      1010\n",
            "           4       0.98      0.99      0.99       982\n",
            "           5       0.96      0.99      0.98       892\n",
            "           6       0.99      0.99      0.99       958\n",
            "           7       0.98      0.98      0.98      1028\n",
            "           8       0.99      0.96      0.97       974\n",
            "           9       0.98      0.97      0.98      1009\n",
            "\n",
            "    accuracy                           0.98     10000\n",
            "   macro avg       0.98      0.98      0.98     10000\n",
            "weighted avg       0.98      0.98      0.98     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on the test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "correct_test, total_test = 0, 0\n",
        "all_predicted, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1) #we look for the label with the highest assigned probability for each image\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "        # Collect predictions and true labels for later analysis\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy_test = correct_test / total_test\n",
        "\n",
        "# Print test accuracy\n",
        "print(f'Test Accuracy: {accuracy_test * 100:.2f}%')\n",
        "\n",
        "# TODO: Calculate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(all_labels, all_predicted)\n",
        "\n",
        "print('\\nConfusion Matrix:')\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate and print classification report\n",
        "class_report = classification_report(all_labels, all_predicted)\n",
        "print('\\nClassification Report:')\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTQieAN-xlmG"
      },
      "source": [
        "**sklearn.metrics.confusion_matrix**: We can use this method to generate a confusion matrix from our data by passing in the true labels (all_labels) and the predicted labels (all_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6JPsSYaWJHe"
      },
      "source": [
        "This code snippet sets the model to evaluation mode, runs the test data through the trained model, and calculates the accuracy. This code snippet sets the model to evaluation mode, runs the test data through the trained model, and calculates the accuracy. The Confusion Matrix compares between what the true labels are, and what our guesses are – 100% accuracy would mean that this comes out with only values on the diagonal. The classification report gives multiple metrics, including precision (what percent of guesses for a specific category are correct), recall (what percent of a certain class are labeled correctly), abd f1 score (a combination of precision and recall)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xxlesHAWvXx"
      },
      "source": [
        "##Step 9: Visualize the Results of our Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "4ch-vsRlY_3p",
        "outputId": "c22644fe-1f22-42de-8e4c-f309a0a3d018"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to visualize test images and predictions\n",
        "def visualize_results(model, test_loader, num_images=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate(test_loader):\n",
        "            if i >= num_images:\n",
        "                break\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Convert the image tensor to a NumPy array\n",
        "            image = images[0][0].squeeze().cpu().numpy()\n",
        "\n",
        "            # Visualize the test images and their predictions\n",
        "            plt.subplot(1, num_images, i + 1)\n",
        "            plt.imshow(image, cmap='gray')  # Use the NumPy array for imshow\n",
        "            plt.title(f'Predicted: {predicted[0]}\\nActual: {labels[0]}')\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Visualize test images and predictions\n",
        "visualize_results(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqy-bLAPZBva"
      },
      "source": [
        "*   **'visualize_results'**: This function takes the trained model and a test data loader as input and visualizes a specified number of test images along with their predicted and actual labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkT3AlTweA5F"
      },
      "source": [
        "#Let's Upgrade Our CNN!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKE_C_k5eHS1"
      },
      "source": [
        "There are several next steps we can take to bring our current CNN to the next level. Two of the most common next steps are updating the CNN architecture and data augmentation. We will be implementing both in the next few steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef9i99_DeWYK"
      },
      "source": [
        "##Step 10: Update CNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o4Zbp3XecOO"
      },
      "outputs": [],
      "source": [
        "# Define the advanced CNN architecture\n",
        "class AdvancedCNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedCNNClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calculate the size dynamically based on the input size to the linear layer\n",
        "        dummy_input = torch.randn(1, 1, 28, 28)\n",
        "        self.calculate_conv_output_size(dummy_input)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.conv_output_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def calculate_conv_output_size(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        self.conv_output_size = x.size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIJmeK-cef5a"
      },
      "source": [
        "We've defined a new CNN architcture named **'AdvancedCNNClassifier'** with three convolutional layers('conv1', 'conv2', 'conv3'), max-pooling layers('pool'), and wo fully connected layers ('fc1', 'fc2').\n",
        "\n",
        "\n",
        "The architecture is more complex than the previous CNN, with additional convolutional and fully connected layers.\n",
        "\n",
        "\n",
        "In this step we've also introduced a method 'calculate_conv_output_size' that takes. adummy input and calculates the size after the last pooling layer. This size is then used to dynamically set the input size of the first linear layer('fc1')."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afPDNpvihc66"
      },
      "source": [
        "To provide some general foundational reasoning for updates in CNN architecture:\n",
        "\n",
        "1. Increased Model Capacity: The original CNN architecture might not have enough capacity to capture intricate patterns in the data. By introducing more convolutional layers and parameters, the model becomes more expressive and can potentially learn more complex features.\n",
        "2. Hierarchical Feature Extraction: Deepening the architecture allows the network to perform hierarchical feature extraction. Each layer can learn and represent different levels of abstraction, enabling the model to understand more intricate structures in the input data.\n",
        "3. Representation Power: The deeper architecture provides the model with a higher capacity to represent both low-level and high-level features in the data. This can be particularly beneficial for image classification tasks where features are hierarchical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUgKYOOZe2z0"
      },
      "source": [
        "##Step 11: Data Augmentaiton\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9hguoOZe58k"
      },
      "outputs": [],
      "source": [
        "# Define data transforms for training with data augmentation\n",
        "transform_with_augmentation = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(28, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download and load the training set with data augmentation\n",
        "train_dataset_with_augmentation = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=transform_with_augmentation,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Create a data loader for the augmented training set\n",
        "train_loader_with_augmentation = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset_with_augmentation,\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PjTyKUdfJNL"
      },
      "source": [
        "We've defined a new set of data transformation('transform_with_augmentation') for training with data augmentation.\n",
        "\n",
        "The transformations include random rotation, random resized crop, and random horizontal flip, augmenting the training images.\n",
        "\n",
        "We've created a new training dataste('train_dataset_with_augmentation') with the specified augmentations.\n",
        "\n",
        "Finally, a new data loader ('train_loader_with_augmentation') is created for the augmented training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ8VPCc_iCWL"
      },
      "source": [
        "To provide some foundational reasoning for why one might want to perform data augmentation:\n",
        "1. Increased Robustness: Data augmentation introduces diversity into the training set by applying random transformations to the images. This helps the model become more robust and less sensitive to variations in the input data.\n",
        "2. Improved Generalization: Augmenting the training data with variations like rotation, scaling, and flipping exposes the model to a broader range of scenarios. This aids in improving the model's ability to generalize well to unseen data.\n",
        "3. Mitigation of Overfitting: Data augmentation acts as a form of regularization by artificially expanding the dataset. This can be particularly helpful in mitigating overfitting, where the model memorizes the training set instead of learning generalizable patterns.\n",
        "4. Realistic Input Variations: By simulating realistic variations that the model might encounter in real-world scenarios, data augmentation helps the model become more adaptive and capable of handling diverse inputs.\n",
        "5. Reudced Dependency on Large Datasets: In scenarios where collecting a large labeled dataset is challenging, data augmentation provides a way to artificially increase the effective size of the training set. This is especially useful for small to moderately sized datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQLZ69CUfz_c"
      },
      "source": [
        "## Step 12: Run Training Loop Again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqY21Ppff3ia",
        "outputId": "dc768eab-5104-4da2-b6dc-cf1bba9e5892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 0.3535, Accuracy: 88.32\n",
            "Epoch [2/5], Loss: 0.1193, Accuracy: 96.19\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the advanced CNN model\n",
        "advanced_model = AdvancedCNNClassifier()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(advanced_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with data augmentation\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    advanced_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train, total_train = 0, 0\n",
        "\n",
        "    for images, labels in train_loader_with_augmentation:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = advanced_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader_with_augmentation)\n",
        "    accuracy_train = correct_train / total_train\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy_train * 100:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtlSpaZwgz3a"
      },
      "source": [
        "We've instantiated the advanced CNN model ('AdvancedCNNClassifier'). The loss function is defined as cross-entropy, and the optimizer is set to Adam witha learning rate of 0.001\n",
        "\n",
        "The training loop is similar to the augmented training dataset ('train_loader_with_augmentation). The loop then prints the training loss and accuracy for each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OLz6SS0ioIw"
      },
      "source": [
        "Incorporating both an advanced architecture and data augmentation aligns with the goal of building a more powerful and robust model, capable of accurately classifying handwritten digits while being less sensitive to variations and noise in the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITQrcByYWPpM"
      },
      "source": [
        "#Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMoZ5lxoWVRj"
      },
      "source": [
        "Congratulations! You've successfully built and trained a Convolutional Neural Network for classifying handwritten digits using PyTorch. In this tutorial, we covered setting up the environment, defining the CNN architecture, training the model, and evaluating its performance.\n",
        "\n",
        "Feel free to experiment with different architectures, hyperparameters, and techniques to further improve the model's accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vekhrxyDO3T"
      },
      "source": [
        "#Optional: Visualizing CNN Feature Maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSpWxDaRKW4M"
      },
      "source": [
        "We can visualize CNN feature maps because each convolutional layer in the network produces a structured output: a 2D grid of numbers (like a grayscale image) that represents how strongly each filter responds to different regions of the input. These outputs, called feature maps or activation maps, are just tensors that we can extract during a forward pass. Since each map has a clear spatial layout and intensity values, we can easily convert them into images, letting us peek inside the network and see what patterns it detects as it processes the input images.\n",
        "\n",
        "Before we can visualize what our CNN is \"seeing,\" we need to tap into the inside of the network. We do this by setting up a hook — a tool that captures the output of a specific layer during a forward pass. Here, we'll hook into the first convolutional layer to see how it responds to input images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVvmSX2NHTVZ"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary to hold the activation\n",
        "activation = {}\n",
        "\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "# Register the hook\n",
        "model.conv1.register_forward_hook(get_activation('conv1'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcI8M8dnMhAH"
      },
      "source": [
        "Now that we can capture the activations, let's write a function to display them alongside the original image. Each feature map shows how one filter in the convolutional layer responds to different regions of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bFCJQI8Hl8y"
      },
      "outputs": [],
      "source": [
        "def visualize_feature_maps(input_tensor, activation, layer_name, num_maps=6):\n",
        "    feature_maps = activation[layer_name].squeeze(0)  # shape: [C, H, W]\n",
        "    num_maps = min(num_maps, feature_maps.shape[0])\n",
        "\n",
        "    # Convert input image to CPU and squeeze batch + channel dims\n",
        "    input_image = input_tensor.squeeze(0).squeeze(0).cpu()\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_maps + 1, figsize=((num_maps + 1) * 2, 2))\n",
        "\n",
        "    # Show original input image\n",
        "    axes[0].imshow(input_image, cmap='gray')\n",
        "    axes[0].set_title(\"Input Image\", fontsize=8)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Show activation maps\n",
        "    for i in range(num_maps):\n",
        "        map_i = feature_maps[i].cpu()\n",
        "        # Normalize for better visual contrast\n",
        "        map_i = (map_i - map_i.min()) / (map_i.max() - map_i.min() + 1e-5)\n",
        "        axes[i + 1].imshow(map_i, cmap='gray')\n",
        "        axes[i + 1].set_title(f\"Map {i}\", fontsize=8)\n",
        "        axes[i + 1].axis('off')\n",
        "\n",
        "    fig.suptitle(f\"Feature Maps from {layer_name}\", fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-3pDNhEMhy6"
      },
      "source": [
        "Now we'll take the first few images from the test dataset, run it through the model, and visualize what the first layer of the CNN detects. White areas mean \"this filter sees something important here.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LxE8huRL3uO"
      },
      "outputs": [],
      "source": [
        "num_inputs = 3\n",
        "num_maps = 6\n",
        "\n",
        "for i in range(num_inputs):\n",
        "    # Load one image from test set\n",
        "    image, label = test_dataset[i]  # Choose index 0 or any digit\n",
        "    input_tensor = image.unsqueeze(0)  # Add batch dimension: [1, 1, 28, 28]\n",
        "\n",
        "    # Ensure model is in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_tensor)\n",
        "\n",
        "    # Visualize\n",
        "    visualize_feature_maps(input_tensor, activation, 'conv1', num_maps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGB6WTg6OY_U"
      },
      "source": [
        "Can you try to interpret what each feature map is looking for? How might the model then use the information it finds? Provide specific, concrete examples."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
